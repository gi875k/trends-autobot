import time
import pandas as pd
import gspread
import pytz
from oauth2client.service_account import ServiceAccountCredentials
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
from datetime import datetime
import requests
import json

# --- 1. Google Trends ìŠ¤í¬ë˜í•‘ í•¨ìˆ˜ ---
def scrape_google_trends():
    """Seleniumì„ ì‚¬ìš©í•˜ì—¬ Google Trendsì˜ ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ë¥¼ ìŠ¤í¬ë˜í•‘í•©ë‹ˆë‹¤."""
    # (ì´ì „ ì½”ë“œì™€ ë™ì¼, ë³€ê²½ ì—†ìŒ)
    url = "https://trends.google.com/trending?geo=KR&hours=4"
    options = webdriver.ChromeOptions()
    options.add_argument('headless')
    options.add_argument('window-size=1920x1080')
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36")
    
    service = Service(ChromeDriverManager().install())
    driver = None
    try:
        print("ğŸ” [Google] Seleniumìœ¼ë¡œ Chrome ë¸Œë¼ìš°ì €ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤...")
        driver = webdriver.Chrome(service=service, options=options)
        driver.get(url)
        print("   - JavaScript ë°ì´í„° ë¡œë”© ëŒ€ê¸° ì¤‘ (3ì´ˆ)...")
        time.sleep(3)
        html_source = driver.page_source
        print("âœ… [Google] í˜ì´ì§€ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸° ì„±ê³µ!")
        
        soup = BeautifulSoup(html_source, 'lxml')
        table_body = soup.find('tbody', jsname='cC57zf')
        if not table_body:
            print("âŒ [Google] ê²€ìƒ‰ì–´ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        rows = table_body.find_all('tr')
        if not rows:
            print("âŒ [Google] ê²€ìƒ‰ì–´ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
            
        keywords = [row.find('div', class_='mZ3RIc').get_text(strip=True) for row in rows if row.find('div', class_='mZ3RIc')]
        print(f"âœ… [Google] ìŠ¤í¬ë˜í•‘ ì„±ê³µ! ({len(keywords)}ê°œ í‚¤ì›Œë“œ ë°œê²¬)")
        return keywords

    except Exception as e:
        print(f"âŒ [Google] Selenium ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None
    finally:
        if driver:
            driver.quit()

# --- 2. ZUM íŠ¸ë Œë“œ ìŠ¤í¬ë˜í•‘ í•¨ìˆ˜ ---
def scrape_zum_trends():
    """requestsì™€ BeautifulSoupì„ ì‚¬ìš©í•˜ì—¬ ZUMì˜ ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ë¥¼ ìŠ¤í¬ë˜í•‘í•©ë‹ˆë‹¤."""
    # (ì´ì „ ì½”ë“œì™€ ë™ì¼, ë³€ê²½ ì—†ìŒ)
    try:
        print("ğŸ” [ZUM] ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ ìŠ¤í¬ë˜í•‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        url = 'https://m.search.zum.com/search.zum?method=uni&option=accu&qm=f_typing.top&query='
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'html5lib')
        list_wrap = soup.find('div', {'class' : 'list_wrap animate'})
        if not list_wrap: return None

        keyword_tags = list_wrap.find_all('span', {'class' : 'keyword'})
        if not keyword_tags: return None
        
        keywords = [k.text.strip() for k in keyword_tags]
        print(f"âœ… [ZUM] ìŠ¤í¬ë˜í•‘ ì„±ê³µ! ({len(keywords)}ê°œ í‚¤ì›Œë“œ ë°œê²¬)")
        return keywords
    except Exception as e:
        print(f"âŒ [ZUM] ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

# --- 3. [ì‹ ê·œ] NATE íŠ¸ë Œë“œ í¬ë¡¤ëŸ¬ í•¨ìˆ˜ ---
def scrape_nate_trends():
    """NATEì˜ ì‹¤ì‹œê°„ ì´ìŠˆ í‚¤ì›Œë“œ JSON ë°ì´í„°ë¥¼ ì§ì ‘ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        print("ğŸ” [NATE] ì‹¤ì‹œê°„ ì´ìŠˆ í‚¤ì›Œë“œ API ìš”ì²­ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        # ìºì‹œ ë°©ì§€ë¥¼ ìœ„í•´ í˜„ì¬ ì‹œê°„ìœ¼ë¡œ íŒŒë¼ë¯¸í„° ìƒì„±
        now = datetime.now().strftime('%Y%m%d%H%M')
        url = f'https://www.nate.com/js/data/jsonLiveKeywordDataV1.js?v={now}'
        
        response = requests.get(url)
        response.raise_for_status()

        # NATEëŠ” euc-kr ì¸ì½”ë”©ì„ ì‚¬ìš©í•˜ë¯€ë¡œ, í•´ë‹¹ ë°©ì‹ìœ¼ë¡œ ë””ì½”ë”©
        data = response.content.decode('euc-kr')
        
        # JSON í˜•ì‹ìœ¼ë¡œ íŒŒì‹±
        keyword_list = json.loads(data)
        
        # ë°ì´í„° êµ¬ì¡°: ['ìˆœìœ„', 'í‚¤ì›Œë“œ', 'ë³€ë™'] -> ì—¬ê¸°ì„œ 'í‚¤ì›Œë“œ'ë§Œ ì¶”ì¶œ
        keywords = [item[1] for item in keyword_list]
        
        print(f"âœ… [NATE] ë°ì´í„° ìˆ˜ì‹  ì„±ê³µ! ({len(keywords)}ê°œ í‚¤ì›Œë“œ ë°œê²¬)")
        return keywords
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ [NATE] ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None
    except json.JSONDecodeError:
        print("âŒ [NATE] JSON íŒŒì‹± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì‘ë‹µ í˜•ì‹ì´ ë³€ê²½ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return None
    except Exception as e:
        print(f"âŒ [NATE] ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

# --- 4. [í™•ì¥] Google Sheet ì—…ë°ì´íŠ¸ í•¨ìˆ˜ ---
def update_google_sheet(google_data, zum_data, nate_data):
    """3ê°œì˜ íŠ¸ë Œë“œ ë°ì´í„°ë¥¼ ì·¨í•©í•˜ì—¬ Google Sheetì— ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
    try:
        print("\nğŸ”„ Google Sheetì— ì—°ê²°ì„ ì‹œë„í•©ë‹ˆë‹¤...")
        scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
        creds = ServiceAccountCredentials.from_json_keyfile_name('credentials.json', scope)
        client = gspread.authorize(creds)
        sheet = client.open("ë¸”ë¡œê·¸ ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´").sheet1
        print(f"   - '{sheet.title}' ì‹œíŠ¸ì— ì—°ê²° ì„±ê³µ!")

        sheet.clear()

        kst = pytz.timezone('Asia/Seoul')
        now_kst = datetime.now(kst)
        timestamp_text = now_kst.strftime('(%mì›” %dì¼ %Hì‹œ ì—…ë°ì´íŠ¸)')
        sheet.update_acell('A1', timestamp_text)
        print(f"âœ… ê¸°ì¤€ ì‹œê°„ '{timestamp_text}'ì„(ë¥¼) A1ì…€ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

        # 3ê°œì˜ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ë¥¼ Pandas Seriesë¡œ ë³€í™˜
        google_series = pd.Series(google_data, name='Google íŠ¸ë Œë“œ')
        zum_series = pd.Series(zum_data, name='ZUM íŠ¸ë Œë“œ')
        nate_series = pd.Series(nate_data, name='NATE íŠ¸ë Œë“œ')
        
        # 3ê°œì˜ Seriesë¥¼ ì˜†ìœ¼ë¡œ ì´ì–´ë¶™ì—¬ í•˜ë‚˜ì˜ DataFrame ìƒì„±
        combined_df = pd.concat([google_series, zum_series, nate_series], axis=1)

        # A2 ì…€ë¶€í„° í—¤ë”ì™€ ë°ì´í„° ì €ì¥
        if not combined_df.empty:
            combined_df.fillna('', inplace=True)
            sheet.update('A2', [combined_df.columns.values.tolist()] + combined_df.values.tolist(), value_input_option='USER_ENTERED')
            print("âœ… Google, ZUM, NATE í‚¤ì›Œë“œ ëª©ë¡ì„ A2ì…€ë¶€í„° ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        else:
            print("âš ï¸ ì—…ë°ì´íŠ¸í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    except FileNotFoundError:
        print("âŒ 'credentials.json' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Google Cloud Consoleì—ì„œ ìƒì„± í›„, ì½”ë“œì™€ ê°™ì€ í´ë”ì— ì €ì¥í•´ì£¼ì„¸ìš”.")
    except gspread.exceptions.SpreadsheetNotFound:
        print("âŒ Google Sheetì—ì„œ 'ë¸”ë¡œê·¸ ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´' ìŠ¤í”„ë ˆë“œì‹œíŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ Google Sheet ì‘ì—… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# --- ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„ ---
if __name__ == "__main__":
    print("=" * 50)
    print("ğŸš€ Google, ZUM, NATE íŠ¸ë Œë“œ ìŠ¤í¬ë˜í•‘ ë° ì‹œíŠ¸ ì—…ë°ì´íŠ¸ ì‹œìŠ¤í…œ ì‹œì‘")
    print("=" * 50)
    
    # 1. ê° í¬í„¸ì˜ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°
    google_keywords = scrape_google_trends()
    print("-" * 50)
    zum_keywords = scrape_zum_trends()
    print("-" * 50)
    nate_keywords = scrape_nate_trends()
    
    # 2. ëª¨ë“  í‚¤ì›Œë“œ ëª©ë¡ì„ êµ¬ê¸€ ì‹œíŠ¸ì— ì €ì¥í•˜ê¸°
    if google_keywords or zum_keywords or nate_keywords:
        update_google_sheet(google_keywords, zum_keywords, nate_keywords)
    else:
        print("\nëª¨ë“  ì†ŒìŠ¤ì—ì„œ í‚¤ì›Œë“œë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í•´ Google Sheetë¥¼ ì—…ë°ì´íŠ¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
    print("\nâœ… ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
